---
title: "Automatic Dimensionality Selection"
author: "Kemeng Zhang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Principle Component Analysis

Principle component analysis, or PCA is a technique that is widely used for applications such as dimensionality reduction. PCA can be defined as the orthogonal projection of the data onto a lower dimensional linear space, know as the principle subspace, such that the variance of the projected data is maximized.

### Solution

The variance will be a maximum when we set each principle component equal to the eigenvector. Here, the corresponding eigenvalue measures the relative importance of each principle component. Dimension reduction is then achieved by selecting only the top few coordinates (principle components).

The problem that we shall focus on now is that of deciding how many coordinates should be retained.

## Dimensionality Selection

The gist of our method consists of explicitly constructing a model for the eigenvalues and estimating the position of the “gap” or the “elbow” by maximizing a profile likelihood function.

### Scree plot
Plot the eigenvalues in descending order (often called a scree plot) and look for a “big gap” or an “elbow” in such a graph.

```{r, fig.height=3.5, fig.width=5}
# Assumes distribution of eigenvalues follow uniform distribution
d1=runif(10,0,10) # First 10 eigenvalues
d2=runif(10,15,25) # Last 10 eigenvalues
d = c(d1,d2)
d = sort(d,decreasing = TRUE)
barplot(
    height = d,
    ylim = c(0,25),
    xlab = "Principle Component", ylab = "Variance", main = "Screeplot")
```

### The Method

First, order measures of importance of the corresponding coordinates. In the case of PCA these are ordered eigenvalues. Our basic idea is very simple. If a “gap” or an “elbow” exists at position N, then we can think of two different sections seperated by position N as samples from two different distributions. For simplicity, we assume these distributions are Gaussian. We then proceed to compute the MLE for N based on the profile likelihood.

## Testing/Simulated Examples
```{r, include=FALSE, echo=FALSE}
require(graphstats)
if (!require(pracma)) {
    install.packages("pracma")
}
require(pracma)
if (!require(stats)) {
    install.packages("stats")
}
require(stats)
if (!require(eigeninv)) {
    install.packages("eigeninv")
}
require(eigeninv)
```

```{r, fig.height=3.5, fig.width=5}
elbows = sort(sample(40,2)) + 5
while (elbows[2] - elbows[1] < 10) {
  elbows = sort(sample(40,2)) + 5
}
eval_1 = sort(runif(elbows[1], min = 50, max = 60),decreasing = TRUE)
eval_2 = sort(runif(elbows[2]-elbows[1], min = 30, max = 40),decreasing = TRUE)
eval_3 = sort(runif(50 - elbows[2], min = 10, max = 20),decreasing = TRUE)
eval = c(eval_1,eval_2,eval_3)
S = eigeninv::eiginv(eval,50,symmetric=TRUE) # Generate covariance matrix based on eigenvalues
X = pracma::randn(1000, 50);
for (i in 1:50) {
  m = mean(X[,i])
  X[,i] = X[,i] - m
}
X = X %*% inv(chol(cov(X)));
X = X %*% chol(S);
pca = princomp(X)
dim = graphstats::dimselect(pca$sdev,n = 2,plot = TRUE)
print(dim)
```

```{r, fig.height=3.5, fig.width=5}
print(elbows)
```

```{r, fig.height=4, fig.width=6}
blue <- rgb(0, 0, 1, alpha=0.2)
red <- rgb(1, 0, 0, alpha=0.5)
space_vector = c(0.9, rep(1.4, 49))
barplot(pca$sdev^2, ylim = c(0, (pca$sdev[which.max(pca$sdev)])^2), width = 1, col = blue, axisnames = FALSE)
barplot(eval,add = TRUE, col = red, ylim = c(0, (eval[which.max(eval)])^2), width = 0.5, space = space_vector, names.arg = c(1:50), main="Compare true eigenvalues and those from data")
legend("topright", 
       legend = c("True eigenvalues", "Simulated"), 
       fill = c(red, blue))
```





